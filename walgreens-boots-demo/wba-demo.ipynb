{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walgreens Boots Alliance Demo\n",
    "\n",
    "## Setup your environment\n",
    "\n",
    "Before running any cells, make sure you have:\n",
    "* Created your conda environment as per the README.md, \n",
    "* Selected your new conda environment to run this notebook (from the Kernel menu).\n",
    "* Updated the values as indicated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "subscription_id = '<insert your Azure subscription ID here>' \n",
    "resource_group  = '<insert the name of the resource group that holds your AML workspace here>'\n",
    "workspace_name  = '<insert your AML workspace name here>'\n",
    "experiment_name = 'category-based-propensity'\n",
    "cluster_name = 'wba-cluster'\n",
    "project_folder = 'scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key open source data analysis packages\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(color_codes='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directories\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring our dataset\n",
    "\n",
    "Let's take a look at our dataset before building our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in to analyze\n",
    "df = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview available columns\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of ages in the dataset\n",
    "sns.distplot(df[['AGE']], bins=[10,20,30,40,50,60,70,80,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of spend in category #1\n",
    "sns.distplot(df[('CATEGORY_1_SPEND')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how age influences whether customers have responded to category #1 campaigns\n",
    "g = sns.FacetGrid(df, col='BOUGHT_CATEGORY_1')\n",
    "g.map(sns.distplot, 'AGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how gender influences whether customers have responded to category #1 campaigns\n",
    "g = sns.FacetGrid(df, col='BOUGHT_CATEGORY_1')\n",
    "g.map(sns.countplot, 'GENDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how age and category #1 & #2 spend influences responding to category #1 campaigns\n",
    "sns.pairplot(df[['AGE', 'CATEGORY_1_SPEND', 'CATEGORY_2_SPEND', 'BOUGHT_CATEGORY_1']], hue='BOUGHT_CATEGORY_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure Machine Learning\n",
    "\n",
    "Let's connect, provision our compute, and upload our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Azure Machine Learning\n",
    "from azureml.core import Run\n",
    "from azureml.core.compute import AksCompute, ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.runconfig import DataReferenceConfiguration, RunConfiguration\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline, PipelineRun, Schedule, TrainingOutput\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.train.automl import AutoMLConfig, AutoMLStep\n",
    "from azureml.train.automl.automlexplainer import retrieve_model_explanation\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import azureml\n",
    "\n",
    "# Connect to Azure Machine Learning\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "except:\n",
    "    ws = Workspace(subscription_id = subscription_id,\n",
    "                   resource_group = resource_group,\n",
    "                   workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    \n",
    "    print('Workspace config file written')\n",
    "    \n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data=output, index=['']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provision a compute target\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "compute_target.status.serialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload our data\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload_files(['./data.csv'], target_path = 'boots', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Automated ML\n",
    "\n",
    "Let's submit a training run using our data and Automated ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_data():\n",
    "    df = pd.read_csv('/tmp/azureml_runs/boots/data.csv')\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['BOUGHT_CATEGORY_1'].values)\n",
    "    y = le.transform(df['BOUGHT_CATEGORY_1'].values)\n",
    "\n",
    "    df = df.drop(['BOUGHT_CATEGORY_1'], axis=1)\n",
    "\n",
    "    return { \"X\" : df, \"y\" : y }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experiment\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our run configuration including our data source reference and base image configuration\n",
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                                path_on_compute='/tmp/azureml_runs',\n",
    "                                path_on_datastore='boots',\n",
    "                                mode='download',\n",
    "                                overwrite=False)\n",
    "\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.target = compute_target\n",
    "run_config.data_references = {ds.name: dr}\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk[automl]','azureml-explain-model'], conda_packages=['numpy','py-xgboost<=0.80'])\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             iterations = 25,\n",
    "                             iteration_timeout_minutes = 5, \n",
    "                             max_cores_per_iteration = 2,\n",
    "                             max_concurrent_iterations = 4,\n",
    "                             primary_metric = 'accuracy',\n",
    "                             data_script = project_folder + '/get_data.py',\n",
    "                             run_configuration = run_config,\n",
    "                             path = project_folder,\n",
    "                             n_cross_validations = 2,\n",
    "                             preprocess = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)\n",
    "remote_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review our results\n",
    "\n",
    "Once the experiment completes, let's review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run details capture configuration and exact Git commit used for the run\n",
    "remote_run_df = pd.read_json('[' + json.dumps(remote_run.get_details()['properties']) + ']', orient='columns')\n",
    "remote_run_df[['azureml.git.branch','azureml.git.commit','azureml.git.repository_uri']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily explore results using interactive widgets\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically find the best model based on different metrics\n",
    "lookup_metric = 'accuracy'\n",
    "best_run, fitted_model = remote_run.get_output(metric = lookup_metric)\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish our model\n",
    "\n",
    "Once we've selected our preferred model, we can register it for management (and optional deployment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the preferred model for your team to use\n",
    "model = best_run.register_model(model_name = 'category_1_model.pkl',\n",
    "                                model_path = 'outputs/model.pkl',\n",
    "                                tags = {'area': 'CATEGORY 1', 'type': 'classification'})\n",
    "print(model.name, model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captures training details\n",
    "model.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline\n",
    "\n",
    "We can build an AML pipeline to make our experiment easy to re-run as data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/register.py\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Variant name you want to give to the model.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model_path',\n",
    "        type=str,\n",
    "        default='outputs',\n",
    "        help='Location of trained model.'\n",
    "    )\n",
    "\n",
    "    args, unparsed = parser.parse_known_args()\n",
    "    print(args.model_name)\n",
    "    print(args.model_path)\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "    \n",
    "    tags = {\n",
    "        \"runId\": str(run.id)\n",
    "    }\n",
    "\n",
    "    print(json.dumps(tags))\n",
    "\n",
    "    model = Model.register(ws, model_name = args.model_name, model_path = args.model_path, tags=tags)\n",
    "\n",
    "    print('Model registered: {} \\nModel Description: {} \\nModel Version: {}'.format(model.name, model.description, model.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use our experiment configuration\n",
    "input_data = DataReference(datastore=ds, \n",
    "                           data_reference_name='training_data',\n",
    "                           path_on_datastore='boots',\n",
    "                           mode='download',\n",
    "                           path_on_compute='/tmp/azureml_runs',\n",
    "                           overwrite=True)\n",
    "\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk[automl]'], conda_packages=['numpy','py-xgboost<=0.80'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline\n",
    "steps = []\n",
    "\n",
    "# These are the two outputs from AutoML\n",
    "metrics_data = PipelineData(name='metrics_data_category_1',\n",
    "                            datastore=ds,\n",
    "                            pipeline_output_name='metrics_output_category_1',\n",
    "                            training_output=TrainingOutput(type='Metrics'))\n",
    "\n",
    "model_data = PipelineData(name='model_data_category_1'.format(cat),\n",
    "                          datastore=ds,\n",
    "                          pipeline_output_name='best_model_output_category_1',\n",
    "                          training_output=TrainingOutput(type='Model'))\n",
    "\n",
    "# AutoML config (note different data files for each model so it's not shared)\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             iterations = 25,\n",
    "                             iteration_timeout_minutes = 5, \n",
    "                             max_cores_per_iteration = 2,\n",
    "                             max_concurrent_iterations = 4,\n",
    "                             primary_metric = 'accuracy',\n",
    "                             data_script = '{}/get_data.py',\n",
    "                             run_configuration = run_config,\n",
    "                             compute_target = compute_target,\n",
    "                             path = project_folder,\n",
    "                             n_cross_validations = 2,\n",
    "                             preprocess = True)\n",
    "\n",
    "# AutoML action\n",
    "automl_step = AutoMLStep(name='automl_module_category_1',\n",
    "                         automl_config=automl_config,\n",
    "                         inputs=[input_data],\n",
    "                         outputs=[metrics_data, model_data],\n",
    "                         allow_reuse=False)\n",
    "\n",
    "# Custom script action to register the model afterwards\n",
    "register_step = PythonScriptStep(name='register_category_1',\n",
    "                                 script_name='register.py',\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=project_folder,\n",
    "                                 arguments=['--model_name', 'category_1_model.pkl', '--model_path', model_data],\n",
    "                                 inputs=[model_data],\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "pipeline = Pipeline(description='Generate recommendation models',\n",
    "                    workspace=ws,\n",
    "                    steps=[automl_step, register_step])\n",
    "\n",
    "pipeline.validate()\n",
    "\n",
    "# Once published, we can invoke on demand via the SDK or via a REST endpoint\n",
    "published_pipeline = pipeline.publish(name='category-based-propensity-pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule our pipeline\n",
    "\n",
    "Now that our experiment is available as a pipeline, we can schedule it or run it on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically run our pipeline when the data changes\n",
    "schedule = Schedule.create(workspace=ws,\n",
    "                           name='category-based-propensity-schedule',\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='category-based-propensity-schedule',\n",
    "                           datastore=ds,\n",
    "                           path_on_datastore='boots',\n",
    "                           wait_for_provisioning=True,\n",
    "                           polling_interval=1,\n",
    "                           description='Scheduled run of category-based-propensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, run it on demand\n",
    "published_pipeline.submit(ws, published_pipeline.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
